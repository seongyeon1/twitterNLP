{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_tweet",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN9xPsUhbc5G8CouekZSt7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seongyeon1/twitterNLP/blob/main/Bert_tweet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 구글 공식 BERT코드와 필요한 패키지 임포트"
      ],
      "metadata": {
        "id": "qdru2_VpWg2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
        "!wget -q https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
        "!wget -q https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n",
        "!wget -q https://raw.githubusercontent.com/google-research/bert/master/tokenization.py"
      ],
      "metadata": {
        "id": "CA3TAy93Wp08"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# !pip uninstall tensorflow==2.8.0\n",
        "# !pip install tensorflow==1.15.0\n",
        "# !pip install bert-tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsmVj5kuY0CO",
        "outputId": "4f2973a3-788b-472a-8d88-920b5660e737"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 패키지 임포트\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import sys\n",
        "import zipfile\n",
        "import modeling\n",
        "import optimization\n",
        "import run_classifier\n",
        "import tokenization\n",
        " \n",
        "from tokenization import FullTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "import tensorflow_hub as hub\n",
        "from tqdm import tqdm_notebook\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "jKMoaxgrWqhL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "903968a1-47df-44a6-8c29-7825910aa2c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6ced56f2e18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/optimization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAdamWeightDecayOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m   \u001b[0;34m\"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT모델과 tokenization의 파라미터 지정\n",
        "sess = tf.Session()\n",
        " \n",
        "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "max_seq_length = 128"
      ],
      "metadata": {
        "id": "ZKtGOX_-WqmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터 로드 및 정제"
      ],
      "metadata": {
        "id": "rLkfvmzBZbkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wMuVd4s4Wqqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_IN_PATH = '/content/drive/MyDrive/ColabNotebooks/datasets/sentiments/'\n",
        "\n",
        "df=pd.read_csv(DATA_IN_PATH + 'preprocessed_df.csv')"
      ],
      "metadata": {
        "id": "ERIju5RVeiQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df[:7920]\n",
        "test_df = df[7920:]"
      ],
      "metadata": {
        "id": "juqaMevNeiLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label']"
      ],
      "metadata": {
        "id": "DEjKOZSQmqgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder().fit(train_df['label'])"
      ],
      "metadata": {
        "id": "pcz0dbmIeiGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_val, X_test = train_df['tweet'].values, test_df['tweet'].values\n",
        "y_train_val = label_encoder.fit_transform(train_df['label'])\n",
        " \n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val,y_train_val, test_size=0.1, random_state=0, stratify = y_train_val\n",
        "        )\n",
        " \n",
        "train_text = X_train\n",
        "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "train_label = y_train\n",
        " \n",
        "val_text = X_val\n",
        "val_text = [' '.join(t.split()[0:max_seq_length]) for t in val_text]\n",
        "val_text = np.array(val_text, dtype=object)[:, np.newaxis]\n",
        "val_label = y_val\n",
        " \n",
        "test_text = X_test\n",
        "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n"
      ],
      "metadata": {
        "id": "93vZUH3Keh9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text"
      ],
      "metadata": {
        "id": "pzfQxPpgnAi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6h5TNtgenBkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. TensorFlow Hub 의 BERT layer 사용하기"
      ],
      "metadata": {
        "id": "OwS6bAsfkzYd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTtgAJE1VL5l"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook\n",
        "#from keras import backend as K\n",
        "from tensorflow.keras.layers import Layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertLayer(Layer):\n",
        "    \n",
        "    def __init__(self, n_fine_tune_layers=10, tf_hub = None, output_representation = 'pooled_output', trainable = False, **kwargs):\n",
        "        \n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.is_trainble = trainable\n",
        "        self.output_size = 768\n",
        "        self.tf_hub = tf_hub\n",
        "        self.output_representation = output_representation\n",
        "        self.supports_masking = True\n",
        "        \n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        " \n",
        "    def build(self, input_shape):\n",
        " \n",
        "        self.bert = hub.Module(\n",
        "            self.tf_hub,\n",
        "            trainable=self.is_trainble,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "        \n",
        "        \n",
        "        variables = list(self.bert.variable_map.values())\n",
        "        if self.is_trainble:\n",
        "            # 1 first remove unused layers\n",
        "            trainable_vars = [var for var in variables if not \"/cls/\" in var.name]\n",
        "            \n",
        "            \n",
        "            if self.output_representation == \"sequence_output\" or self.output_representation == \"mean_pooling\":\n",
        "                # 1 first remove unused pooled layers\n",
        "                trainable_vars = [var for var in trainable_vars if not \"/pooler/\" in var.name]\n",
        "                \n",
        "            # Select how many layers to fine tune\n",
        "            trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "            \n",
        "            # Add to trainable weights\n",
        "            for var in trainable_vars:\n",
        "                self._trainable_weights.append(var)\n",
        " \n",
        "            # Add non-trainable weights\n",
        "            for var in self.bert.variables:\n",
        "                if var not in self._trainable_weights:\n",
        "                    self._non_trainable_weights.append(var)\n",
        "                \n",
        "        else:\n",
        "             for var in variables:\n",
        "                self._non_trainable_weights.append(var)\n",
        "                \n",
        " \n",
        "        super(BertLayer, self).build(input_shape)\n",
        " \n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        \n",
        "        if self.output_representation == \"pooled_output\":\n",
        "            pooled = result[\"pooled_output\"]\n",
        "            \n",
        "        elif self.output_representation == \"mean_pooling\":\n",
        "            result_tmp = result[\"sequence_output\"]\n",
        "        \n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            pooled = masked_reduce_mean(result_tmp, input_mask)\n",
        "            \n",
        "        elif self.output_representation == \"sequence_output\":\n",
        "            \n",
        "            pooled = result[\"sequence_output\"]\n",
        "       \n",
        "        return pooled"
      ],
      "metadata": {
        "id": "5yTDuegZVeJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mask(self, inputs, mask=None):\n",
        "  if self.output_representation == 'sequence_output':\n",
        "    inputs = [K.cast(x, dtype=\"bool\") for x in inputs]\n",
        "    mask = inputs[1]\n",
        "    return mask\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "\n",
        "def compute_output_shape(self, input_shape):\n",
        "  if self.output_representation == \"sequence_output\":\n",
        "      return (input_shape[0][0], input_shape[0][1], self.output_size)\n",
        "  else:\n",
        "      return (input_shape[0][0], self.output_size)"
      ],
      "metadata": {
        "id": "ouVXDrNGVfrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 모델 빌드"
      ],
      "metadata": {
        "id": "HIh-z8hdcXi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        " \n",
        "def build_model(max_seq_length, tf_hub, n_classes, n_fine_tune): \n",
        "    in_id = keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    bert_output = BertLayer(n_fine_tune_layers=n_fine_tune, tf_hub = tf_hub, output_representation = 'mean_pooling', trainable = True)(bert_inputs)\n",
        "    drop = keras.layers.Dropout(0.3)(bert_output)\n",
        "    dense = keras.layers.Dense(256, activation='sigmoid')(drop)\n",
        "    drop = keras.layers.Dropout(0.3)(dense)\n",
        "    dense = keras.layers.Dense(64, activation='sigmoid')(drop)\n",
        "    pred = keras.layers.Dense(n_classes, activation='softmax')(dense)\n",
        "    \n",
        "    model = keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    Adam = keras.optimizers.Adam(lr = 0.0005)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam, metrics=['sparse_categorical_accuracy'])\n",
        "    model.summary()\n",
        " \n",
        "    return model\n",
        " \n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)\n",
        "\n",
        "n_classes = len(label_encoder.classes_)\n",
        "n_fine_tune_layers = 48\n",
        "model = build_model(max_seq_length, bert_path, n_classes, n_fine_tune_layers)\n",
        " \n",
        "# Instantiate variables\n",
        "initialize_vars(sess)\n"
      ],
      "metadata": {
        "id": "1XI5E3a5Vf4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 토큰화(Tokenization)"
      ],
      "metadata": {
        "id": "DEpuWx9KpIw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        " \n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        " \n",
        "def create_tokenizer_from_hub_module(tf_hub):\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.Module(tf_hub)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
      ],
      "metadata": {
        "id": "zVq0J2ZApHAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PaddingInputExample(object):\n",
        "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "UlylTEWHqfnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label = 0\n",
        "        return input_ids, input_mask, segment_ids, label\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "    \n",
        "    #print(tokens)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "    return input_ids, input_mask, segment_ids, example.label"
      ],
      "metadata": {
        "id": "dHbW4loUpHrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        " \n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        np.array(labels).reshape(-1, 1),\n",
        "    )\n",
        " \n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    InputExamples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
        "        )\n",
        "    return InputExamples\n"
      ],
      "metadata": {
        "id": "ZHnumC7apIDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 초기화\n",
        "tokenizer = create_tokenizer_from_hub_module(bert_path)\n",
        " \n",
        "# 데이터를 입력포멧으로 변환\n",
        "train_examples = convert_text_to_examples(train_text, train_label)\n",
        "val_examples = convert_text_to_examples(val_text, val_label)\n",
        " \n",
        "# 피쳐로 변환\n",
        "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
        ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
        "(val_input_ids, val_input_masks, val_segment_ids, val_labels\n",
        ") = convert_examples_to_features(tokenizer, val_examples, max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "yMli82ZBpQwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 모델 훈련"
      ],
      "metadata": {
        "id": "faLfNsj3pVYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        " \n",
        "BATCH_SIZE = 256\n",
        "MONITOR = 'val_sparse_categorical_accuracy'\n",
        "print('BATCH_SIZE is {}'.format(BATCH_SIZE))\n",
        "e_stopping = EarlyStopping(monitor=MONITOR, patience=3, verbose=1, mode='max', restore_best_weights=True)\n",
        "callbacks =  [e_stopping]\n",
        " \n",
        "history = model.fit(\n",
        "   [train_input_ids, train_input_masks, train_segment_ids], \n",
        "    train_labels,\n",
        "    validation_data = ([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
        "    epochs = 10,\n",
        "    verbose = 1,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    callbacks= callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "5sRPf9FApSGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "SukeiRT9k9VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = bert_encoder(test.text.values, tokenizer, max_len=160)\n",
        "test_pred = final_model.predict(test_input)\n",
        "prediction = np.where(test_pred>.5, 1,0)"
      ],
      "metadata": {
        "id": "0PcewoQzlJ5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_input = tokenizer.encode(test_text,\n",
        "\n",
        "truncation=True,\n",
        "\n",
        "padding=True,\n",
        "\n",
        "return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "8JpaH3a2qyRu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}